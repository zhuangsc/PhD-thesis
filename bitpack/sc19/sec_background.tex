\section{Training Deep Neural Networks on Multi-GPU Environments}
\label{sec:background}
DNNs training process typically requires applying  
backpropagation~\cite{Werbos74}, which involves solving a 
large optimization problem.
In this context, the Stochastic Gradient Descent (SGD) algorithm~\cite{KieferWolfowitz1952}, 
which computes the gradient of the cost function of an artificial neural 
network with respect to its weights, is commonly applied.
Each iteration of SGD processes a set of tens or hundreds of samples called batch.
When applying the SGD algorithm, the value of the gradient is updated by combining 
the contributions of all samples contained within a single batch.
By organizing the training samples into batches and just updating the gradient 
values at the end of each batch, a largely parallel procedure is obtained since 
all samples in each batch can be processed in an independent way.

The use of heterogeneous nodes composed of multicore CPU and GPU devices 
is becoming prominent to train DNNs due to the large amount of 
data-level parallelism that such process involves~\cite{You17}.
At the beginning of each iteration, network weights are updated in the 
multicore CPU device and sent to the GPUs.
The different samples of the batch that correspond to the current iteration are 
distributed across the GPUs and processed in a parallel way.
Processing each sample requires running several times highly parallel numerical kernels like the 
GEneral Matrix-Matrix (GEMM) multiplication, which fit very well with GPUs architecture.
Once all samples are processed, their respective contributions to the gradient 
are sent back to the multicore CPU device, which uses them to update the gradient and  
readjust the values of the weights~\cite{jeff12}.
A new set of weights is then sent to the GPUs and a new batch of samples is processed. 
This process is repeated until the neural network provides 
satisfactory results with respect to some test data.
%{\bf There is an alternative method for updating network parameters on each GPU independently. 
%While this second method has been successfully applied in certain scenarios~\cite{Goyal17}, it requires a costly allreduce operation before updating the parameters on each GPU independently.}

This sequence of data exchanges involving different GPUs requires large bandwidth capacity and may constitute a
fundamental performance bottleneck.
%Each time a batch of training samples is processed, a large set of data in terms 
%of neural network parameters and training samples
%are sent  
%to all the GPU devices involved in the parallel run. 
%Although it is possible to increase communication and computation overlap by applying optimizations like pipelining, data motion constitutes an important performance bottleneck.
Section~\ref{sec:performance} provides a performance profile of the training process and shows how data transfers to the GPU require a very significant amount of time.
%which compute the samples contribution to the 
%gradient, which are sent back to the host.  
In this context, our approach consists in reducing as much as possible the amount 
of data sent to the GPUs every time a batch of samples is processed.
This is achieved by reducing 
the number of bits required to represent each network weight. 
While previous approaches exploit reduced data formats to speed up arithmetic and reduce memory requirements~\cite{Micikevicius2018}, we improve performance by compressing data before sending them across multi-GPU systems.
The ADT procedure carries out the compression process.
To drive data compression we use the AWP algorithm, which defines data representation requirements per each network layer.
%To exploit the information provided by AWP we use the ADT procedure, 
%which efficiently compresses DNNs weights. 
Combining both ADT and AWP produces a significant performance gain, which is reported by Section~\ref{sec:evalutation}.

