\section{Related work}
\label{sec:Relatedworks}
A rich body of literature exists describing the effects of using  
data representation formats smaller than the 32-bit Floating Point standard while training neural networks. 
Previous work provides theoretical analysis on the  
ability to learn under limited-precision scenarios of simple networks~\cite{holi93}. 
In recent years, researchers have shown that fixed-precision arithmetic is well suited 
for deep neural networks training~\cite{courbariaux14}, particularly when combined with
stochastic rounding~\cite{gupta15}. 
New data representation formats targeting dynamic and low accuracy opportunities for deep learning have been proposed~\cite{flexpoint17}.
While these approaches have a very large potential for reducing DNNs training costs, 
they do not target 
the data movement problem and, as such, they are orthogonal to the approach 
presented by this paper.

There is a methodology for training deep neural models using 16-bit FP numbers without modifying hyperparameters or losing network accuracy~\cite{Micikevicius2018}.
This previous approach avoids losing accuracy by keeping a 32-bit copy of weights,
scaling the loss function to preserve small gradient updates, and
using 16-bit arithmetic that accumulates into single-precision registers.
%The approach we present in this paper also uses different numerical representation.
This previous approach exploits the tolerance of DNN to data representation formats smaller than the 32-bit FP standard, as our proposal does.
However, our goal is fundamentally different since we reduce data motion in the context of heterogeneous high-end architectures while this previous approach aims at reducing the computing and storage costs of DNN training.
This approach can be combined with \textit{$A^2$DTWP} by decompressing network weights to half-precision to reduce GPU computing time.
This reduction would increase the impact of data motion in the overall performance, which imples that the benefits of \textit{$A^2$DTWP} could be evern larger. 

There are several proposals aimed at improving the Stochastic Gradient Descent (SGD) method like the
Asynchronous SGD~\cite{jeff12} and its variants~\cite{hogwild, 
zhang14}. 
Other approaches \cite{coates13, quoc11} exploit 
model parallelism instead of data-level parallelism to orchestrate large-scale parallel executions of deep learning workloads. 
If the different parallel instances of this model-level parallel scheme had 
different precision requirements, it would be possible to apply approaches close to the ones we present in this paper.

Some previous approaches reduce networks storage and energy requirements to run inference  
on mobile devices~\cite{Han15}. 
While these approaches achieve very large storage reductions, they target inference on embedded systems with limited hardware resources.
%Their network compression techniques are too costly for appyling them many times during training with minimal overhead.
Other approaches achieve large gradient compression ratios in the context of distributed training in mobile devices~\cite{Lin18}.
These approaches achieve very remarkable speedups in low network bandwidth environments and
are applied to scenarios that require frequent and costly allreduce communications.
While they are very valuable in the mobile computing arena, the scope of these approaches is not high-performance computing. 

Several approaches target synchronization costs of SGD gradient updates in the context of parallel executions.
They either quantize gradients to ternary levels \{-1, 0, 1\} to
reduce the overhead of gradient synchronization~\cite{sgd0}, or they propose a family of algorithms allowing for lossy
compression of gradients called Quantized SGD (QSGD)~\cite{sgd1}.
Techniques based on sparsifying gradient updates by removing the 
smallest gradients by absolute value~\cite{sgd2} can also reduce SGD synchronization costs.   
While these approaches apply techniques based on small data representation formats to reduce the synchronization costs of SGD gradient updates, \textit{$A^2$DTWP} targets the cost of sending DNNs weights to the GPU accelerators.
Therefore, these approaches are orthogonal to \textit{$A^2$DTWP} and can be combined with it to reduce as much as possible training communication cost. 
In particular, techniques targeting synchronization costs of SGD gradient updates can be used to reduce GPU to CPU data transfer overhead while \textit{$A^2$DTWP} targets CPU to GPU communication cost.
Therefore, the combination of \textit{$A^2$DTWP} with techniques targeting synchronization costs of SGD gradient updates would reduce both CPU to GPU and GPU to CPU communication overhead.

To the best of our knowledge, this paper is the first in accelerating the training of deep neural networks in multi-GPU high-end systems by reducing data motion.
Our method combines the use of an 
algorithm to dynamically change DNNs weights data representation format during 
training with a highly tuned data compression and decompression procedure. 
Our solution successfully reduces data motion and achieves a significant performance improvement on cutting-edge high-end systems.
While there are previous proposals exploiting mixed precision scenarios to accelerate training, they are orthogonal to our approach as they speed up arithmetic and reduce memory footprint.
Importantly, our proposal can be combined with these previous approaches to obtain a highly optimized training method that minimizes data transfers and accelerates arithmetic in the context of multi-GPU systems. 
%Using this kind of nodes for DNN training is becoming a very common practice in the parallel computing context~\cite{You17}.

