Many relevant High Performance Computing (HPC) applications have to deal with linear systems derived from using discretization schemes like the finite differences or finite elements methods to solve Partial Differential Equations (PDE).
Typically, such discretization schemes produce large matrices with a significant degree of sparsity.
Direct methods like the LU or the QR matrix factorizations are not applicable to such large matrices due to the significant number of steps
they require to fully decompose them.
Iterative methods are a much better option in terms of computational cost and, in particular, Krylov subspace methods are among the most successful ones.
The basic idea behind Krylov methods when solving a linear system $Ax=b$ is to build a solution within the Krylov subspace composed of several powers of matrix $A$ multiplied by vector $b$, that is, $\{b, Ab, A^2b, ..., A^mb\}$. 

The fundamental linear operations involved in Krylov methods are the sparse matrix-vector (SpMV) product, the vector-vector addition and the dot-product.
The performance of the sparse matrix-vector product is strongly impacted by irregular memory access patterns 
%when accessing the cells of the vector, which are 
driven by the irregular positions of the sparse matrix's non-zero coefficients. 
As such, SpMV is typically an expensive memory-bound operation that benefits from large memory bandwidth capacity and also from high-speed interconnection networks. 
The vector-vector additions involved in Krylov methods typically have strided and regular memory access patterns and benefit a lot from resources like memory bandwidth and mechanisms like hardware pre-fetching to enhance their performance.
Finally, the dot-product kernels involve expensive parallel operations like global communications and reductions that constitute an important performance bottleneck when running Krylov subspace methods~\cite{doe}.
%Aspects like OS noise, variance in the memory access latencies due to NUMA effects or system jitter make th

Taking into account the performance aspects of the most fundamental linear algebra kernels of Krylov subspace methods, there are some natural improvements that are described in detail in the literature.
For example, reducing the number of global reductions required by Krylov methods is a well-known option~\cite{rosendale83, Barret94}. 
Indeed, several variations of the Conjugate Gradient (CG) algorithm have been suggested to reduce the number of global dot-products to just one~\cite{chronopoulos89, Saad84, Meurant87, Azevedo93}.
There is also work focused on reducing the number of global synchronizations targeting other subspace Krylov methods (e. g. BiCG and BiCGStab) ~\cite{Yang02, Yang02-1, ghysels13, chronopoulos91}. 
S-step Krylov methods also aim at reducing the number of global reductions~\cite{vorst95, newton95, wayne91, hoemmen}.  
Besides reducing the number of global synchronization points, another alternative to boost Krylov subspace methods performance is to overlap the two most expensive kernels (SpMV and dot-product) either with other computations or between them.
Indeed, overlapping the two dot-products of the CG algorithm with the residual update has already 
been proposed~\cite{Demmel93}, as well as an asynchronous version of the CG algorithm to overlap one of the global reductions with the SpMV and the other with the preconditioner~\cite{gropp10}. 
A variant of the CG algorithm that performs the two global reductions per iteration at once and also hides its latency by overlapping them with the SpMV kernel has also been proposed~\cite{ghysels14}.
Despite this extensive body of work devoted on improving the CG algorithm, performance enhancements brought by state-of-the-art approaches are still far from providing good scalability results~\cite{ghysels14}. 
%For example, CG formulations that have just one synchronization point per iteration and also overlap it with the SpMV kernel are reported to achieve a speedup of 13x when run on 20 cores while classical CG just achieves speedups of 6x~\cite{ghysels14}. 
%Even though this is a remarkable improvement, the achieved scalability is still very far from the linear one~\cite{ghysels14}.  

In this work we propose the \emph{Iteration-Fusing Conjugate Gradient (IFCG)} method, a new formulation of the CG algorithm that outperforms the existing proposals by applying a scheme that aggressively overlaps iterations, which is something not considered by previous work. 
Our approach 
%effectively hides the cost of global synchronization points 
does not update the residual at the end of each iteration and splits numerical kernels into subkernels to relax data-dependencies.
By carrying out these two optimizations, our approach allows computations belonging to different iterations to overlap if there are no specific data or control dependencies between them.
%It also splits the numerical kernels into subkernels to increase performance and relax data-dependencies.
This chapter provides two algorithms that implement the IFCG concept: 
IFCG1, which aims at hiding the costs of global synchronizations and IFCG2, which starts computations as soon as possible to avoid idle time. 

From the programming perspective, there are several ways to enable the overlap of different pieces of computation during a parallel run.
For example, such overlaps can be expressed at the parallel application source code level by using sophisticated programming techniques like pools of threads or asynchronous calls~\cite{Bienia08}.
Other alternatives conceive the parallel execution as a directed graph where nodes represent pieces of code, which are named tasks, and edges represent control or data dependencies between them.
Such approaches require the programmer to annotate the source code in order to express such dependencies and let a runtime system orchestrate the parallel execution.
%A runtime system then orchestrates the parallel execution by tracking dependencies between tasks and scheduling them to run on the available cores once all the input dependencies are satisfied.
In this way, the maximum available parallelism is dynamically extracted without the need for specifying suboptimal overlaps at the source code level.
Approaches based on tasks are becoming important in the parallel programming area.
Indeed, commonly used shared memory programming models like OpenMP include advanced tasking constructs~\cite{OpenMP4.0} and it is also possible to run task-based workloads on distributed memory environments~\cite{Bueno13}.
 
This chapter adopts this task-based paradigm and applies it to the IFCG1 and IFCG2 parallel algorithms. 
Specifically, this chapter improves the state-of-the-art by doing the following contributions:
\begin{itemize}
       \item The Iteration-Fusing Conjugate Gradient (IFCG) approach, which aims at aggressively overlapping different iterations. IFCG is implemented by means of two algorithms: IFCG1 and IFCG2. 
       \item A task-based implementation of the IFCG1 and IFCG2 algorithms that automatically overlaps computations from different iterations without the need for explicit programmer specification on what computations should be overlapped.
       \item A comprehensive evaluation comparing IFCG1 and IFCG2 with the most relevant state-of-the-art formulations of the CG algorithm: Chronopoulos' CG~\cite{chronopoulos89}, Gropp's CG~\cite{gropp10}, Pipelined CG~\cite{ghysels14} and a basic Preconditioned CG method. All 6 CG variants are implemented via a task-based programming model to provide a fair evaluation. IFCG1 and IFCG2 provide parallel performance improvements up to 42.9\% and 41.5\% respectively and average improvements of 11.8\% and 7.1\% with respect to the state-of-the-art techniques and show similar numerical stability.
	\item A demonstration that under realistic system noise regimes IFCG algorithms behave much better than previous approaches. IFCG algorithms achieve an average 18.0\% improvement over the best state-of-the-art techniques under realistic system noise regimes.
\end{itemize}
 
This chapter is structured as follows: 
In section~\ref{sec:ifcg_existingCG} we describe in detail some state-of-the-art approaches that motivate the IFCG algorithms.
Section~\ref{sec:ifcg_ifcg} contains a detailed description of the IFCG1 and IFCG2  algorithms.
Section~\ref{sec:ifcg_stability} compares the numerical stability of IFCG1 and IFCG2 with other relevant state-of-the-art techniques.
Section~\ref{sec:ifcg_implementation} explains how task-based parallelism is applied to IFCG1 and IFCG2 and how they are executed in parallel.
Section~\ref{sec:ifcg_setup} describes in detail the experimental setup of this chapter.
Section~\ref{sec:ifcg_results} shows a comparison of IFCG1 and IFCG2 against other state-of-the-art techniques when run on a 16-core node composed of two 8-core sockets. 
It also discusses other important aspects like the inter-iteration overlap achieved by IFCG1 and IFCG2 and a comparison of the system jitter tolerance of these algorithms against state-of-the-art approaches.
Finally, section~\ref{sec:ifcg_conclusions} contains several conclusions on this work and describes future directions.
