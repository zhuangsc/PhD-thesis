The most direct way of managing the power a processor consumes, is with DVFS.  Vendors 
typically allow their processors to operate at different frequency levels.  Lower frequencies
offer lower power consumption and DVFS can be set and managed by software.  Different cores
can also operate at different frequencies.
However, processor vendors, realizing the importance of managing power in future and current
HPC systems, are starting to offer greater flexibility by allowing users to set the exact 
power limit a processor can reach. 


The ability to set up power bounds in many-core systems is becoming a common feature.
For example, Intel introduced a set of machine-specific registers (MSRs)~\cite{libmsr} on their Sandy/Ivy Bridge processors to explicitly constrain on-chip power consumption. 
Although this seems as a straight-forward solution to managing power, system administrators must
consider manufacturing variability.  Processors do not consume the same power, even though they
are designed to do so.
In order to provide homogeneous performance,
chips of the same architecture must hide frequency variability, which can only be achieved via variations in their power consumption.
To abide to this user-set constraint, CPU cores resort to reducing their frequency. Under a power constraint however, different chips operate under different frequencies.
Since the release of commodity chips with such capabilities, several studies have shown the impact power capping can have. %have been made on power capped systems. 
In particular, work by Rountree et al.~\cite{Rountree2012} motivates the research presented in this 
thesis on how processor performance variability due to power capping can be addressed. 


In a power constrained environment where all chips need to operate under a certain power cap, this frequency variability can no longer be hidden~\cite{Rountree:2012:BDF:2357488.2357648}, leading to heterogeneous performance.
As a result, a theoretically homogeneous system turns into a heterogeneous one with performance variations of up to 64\%~\cite{Inadomi:2015:AMI:2807591.2807638}.  
While ignoring this manufacturing variability leads to performance 
and energy inefficiencies, 
there are opportunities for achieving improvements at the power budgeting or parallel runtime system levels when variability is properly managed~\cite{Chasapis:2016:RMM:2925426.2926279,Teodorescu:2008:VAS:1381306.1382152,Inadomi:2015:AMI:2807591.2807638,Gholkar:2016:PTH:2967938.2967961,Totoni:tech:2014}.
%Currently, state-of-the-art power-aware job scheduling approaches do not consider manufacturing variability %or process variation 
%to manage jobs dispatched across a parallel system.


Inadomi et al.~\cite{Inadomi:2015:AMI:2807591.2807638} also study the performance variability on a number production clusters and propose a variation-aware power budgeting framework.
Their approach requires specific single core executions for profiling the HPC applications plus a once-per-system profiling to build a reference table containing performance variability information for all nodes.
This table and the single core profiling is used to make decisions using a model. 
Compared to their method, our runtime approach does not require dedicated profiling runs or 
system wide reference tables containing performance variations.  Instead, we use profiling
information obtained at runtime to adjust power distribution and concurrency levels, 
which reduces the analysis costs and increases its benefits.

Bailey et al.~\cite{Bailey:2015:FLP:2807591.2807637} propose a linear programming formulation for MPI+OpenMP programs for maximizing performance under job-level power constraints. While this approach provides a good approximation of the upper bound of possible performance in dynamic runtime systems, the use of a linear programming solver is too slow to be practical for optimizing applications at runtime.
%, since the linear programming solver is too slow. 
The same group also introduced Conductor~\cite{conductor2015}, a dynamic runtime system that directs power to the critical path of the computation to minimize overall execution time under a power cap. Conductor, however, does not  
%While Conductor offers an online solution for finding optimal power configurations by exploiting the critical path of MPI applications, it does not 
deal with the hardware manufacturing variability we describe in this work.
As such, our approach is orthogonal and can be combined to maximize parallel applications performance.
%change the active core balance of the multi-core sockets involved in the parallel execution.
%Since such active core balance are an important factor to maximize performance under power constrained scenarios, our method extends the conductor approach and provides more benefits.
%at the runtime level.  
%However, it targets MPI applications, while our method focuses on OpenMP.

On a single node, Cochran et al.~\cite{Cochran:2011:PCA:2155620.2155641} classify the PARSEC benchmark suite applications for their power, temperature and performance characteristics. Using these results, they maximize performance while meeting power constraints by
%They propose a mechanism for maximizing performance and meeting power constrains 
using thread packing and DVFS. In contrast to our runtime approach, though, they rely on their priori characterization, while our approach can work without prior information.
%Optimal configurations are chosen using classification data they have collected a priori.
%our method 
%
%does not required any previoulsy collected data to maximize performance under a given power bound.

%On the level of system-wide power contraints, 
There is a significant body of work focused on job scheduling for power constrained systems.
Etinski et al.~\cite{5999809} propose an LP-based job scheduling policy; % is proposed in 
Sarood et al.~\cite{Sarood:2014:MTO:2683593.2683682} use performance modeling 
%is employed 
to make job scheduling decisions in power constraint system to improve job throughput;
and  Ellsworth et al.~\cite{Ellsworth:2015:PSD:2749246.2749277} discuss a dynamic job scheduling algorithm, which when running under a system-wide power limit, detects unused power and redistributes it to nodes that can make use of it.
%measures power 
%consumption of jobs 
%
%and based on a heuristic method reallocates power to sockets, to maximize performance.  

The impact of manufacturing quality on power consumption variability of processor chips has been studied in a significant number of works as well.
The power leakage of processors is directly connected to our work, since by setting a power limit on the socket, we impair its ability to adjust power consumption
to maintain the proper frequency level.
Davis et al.~\cite{61401478} study the effect of inter-node variability on power model characterization, in the context of homogeneous clusters.
Herbert et al.~\cite{Herbert:2012:EPV:2490159.2490164} show that exposing the power leakage variability of processors to the DVFS control algorithm to shift work to the less leaky processors, can reduce overall system power consumption.
Further, several projects study %Other works %have also taken into account 
the on-die power variation to improve DVFS scheduling~\cite{4919634,4798265,Teodorescu:2008:VAS:1381306.1382152}.
As an additional concern, modern processors require transistors to shrink to a level that introduces significant power and reliability variations among processors, a phenomena explored in detail by a variety of groups~\cite{Borkar:2003:PVI:775832.775920, 915379, 1046081}.
%Power and reliability variations in modern processors due to shrinking transitors sizes have been detailedly explored
Overall, most studies conclude that power variation is expected to become worse in the future~\cite{1382598,915379}, which will make the effects of power budgeting more apparent.  Thus, a 
variability-aware software solution is imperative in managing complex parallel applications and 
improving both performance and energy efficiency. 


