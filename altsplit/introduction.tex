Deep Neural Networks (MLPs, CNNs etc.) have seen a mass adoption into the
industry in recent years~\cite{Acoustic, Language, Ciregan2012}. DNNs provide
very competitive pattern detection capabilities and, more specifically,
Convolutional Neural Networks~(CNNs) classify very large image sets with
remarkable accuracy~\cite{Krizhevsky2012}. 

As DNNs are gaining traction in more and more fields, the needs to accelerate
the otherwise notoriously slow training has become a prominent topic in the
HPC (high performance computing) community. (\textcolor{red}{references})
Furthermore, with the ever-increasing size of the datasets and the
ever-growing complexity of the DNN architecture, nowadays it takes HPC
clusters to train DNNs to reach a competitive accuracy. A simple yet
prevalent method to accelerate the training is to use \emph{data parallelism}
~\cite{model1, pserver} in which the input data are distributed onto various
available computational units (CPUs, GPUs, FPGAs etc.) and the training on
different portion of the data are being carried out simultaneously.
Nevertheless, it does not tackle the problem of the architectural complexity
of the DNNs where the memory capacity of a computational unit is not
sufficient to hold the parameters of the entire network. It is then natural
to develop ways to distribute the network onto multiple computational units.
\emph{Model parallelism} is thus the parallelism paradigm to this
end~\cite{model0,model1}.

Unlike \emph{data parallelism} where the trainings on portions of data have
no inter-dependencies, \emph{model parallelism} inevitably introduces
dependencies among the computational units. As a consequence, communication
will have to occur so that each computational unit is updated with the contribution
from the rest of the units. 