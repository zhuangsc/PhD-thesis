In the last few years, processor clock frequencies have
stagnated, while exploiting instruction-level parallelism
(ILP) has already reached the point of diminishing returns.
Moreover, design complexity and high power requirements
encouraged engineers and researchers to shift to
multi-core designs \cite{Kalla:2004:IPC:1435718.1437666,Kongetira:2005:NMS:1069597.1069758,RamanathanIntelMulticore}, aiming to overcome some of these technological constraints.  
Simpler, power efficient cores would be tiled together on a single chip.  
To exploit this new paradigm however, programmers need to extract parallelism
at application level.  The most common and successful approach
has been Thread Level Parallelism (TLP).  Parallel work is 
divided into multiple entities and run concurrently as different threads of execution.


However, both designing and programming effectively this machines surface new challenges.
First, parallel programming is much harder than programming a uniprocessor chip, 
which is commonly referred as the Programmability Wall \cite{Chapman:2007multicore}.  Second, as cores compete 
for the shared system resources they can saturate them.  The most common case is the contention
of the shared memory elements.  The contention can be caused by a number of factors, for exampe
memory bandwidth can be saturated easier if it needs to service multiple cores or cores competing 
for the same space will result in higher miss rates, thus latency.  This is known as the Memory Wall \cite{Hennessy:2006:CAF:1200662, Mahapatra:1999:PBP:357783.331677}.  
Dealing with both issues will be even harder in the near 
future as industry trends \cite{RamanathanIntelMulticore} indicate that the number of cores will increase with every processor generation.

Memory is not the only resource which is contested for. One major constraint of future High-Performance Computing (HPC) systems 
is their power consumption, also known as the Power Wall.  Agencies have set strict targets for building an exascale machine --- e.g.,  the US Department of Energy has set the limit to 
20MW~\cite{Ashby2010} --- while others, like the European Union, are investing in novel approaches 
leveraging mobile technologies to build low-power HPC infrastructures~\cite{rajovic2013}.
The resulting need for reducing hardware power consumption has started to force computer architects and vendors to include power capping capabilities in their hardware designs. 
This allows applications to more efficiently exploit their entire power envelope of a system, while  guarding the system against intermediate power spikes. 
Prior work has shown that this can lead to significant performance benefits~\cite{patki:2013:eho:2464996.2465009,conductor2015}.
Exposing power control to the user level adds to the complexity of programming a parallel application.


To address the Programmability Wall different programming models have been suggested.  Each offers some form 
of abstraction that allows the programmer to express parallel sections.  The simplest one is the thread model (pthreads~\cite{Butenhof:1997:PPT:263953},
where the programmer can define the work that each thread is going to execute.  In this model it is the developer's
responsibility to manage threads and synchronize accesses to shared data.  Moreover, fine-tuning the application for the given
hardware again up to the developer, which implies that limitations forced by the memory characteristics should be considered 
when developing the application. All these result in poor code protability and enormous effort from the developers part.   


More sophisticated models, such as
OpenMP~\cite{Chapman:2007:UOP:1370966}) is an alternative model to thread that excels at parallelizing program loops, while hiding away details from 
user, by handling thread management with a dedicated runtime system.  More recent versions of OpenMP implement task Parallelism, which offers  
an even higher level of abstraction.  Many other systems implement task parallel models \cite{4090178, Blumofe:PPoPP1995, Ayguade:TPDS2009, Tzenakis:2012:BBD:2370036.2145864, Jenista:2011:OSO:2038037.1941563, 6099829, Duran:PPL2011}.
Parallel work is organized into tasks that can be asynchronously executed.  
Managing tasks is usually the responsibility of the underlying runtime systems which deals with load balancing and thread allocation,
and in some cases even with synchronization~\cite{DuranIJPP09,Tzenakis:2012:BBD:2370036.2145864,Duran:PPL2011}.  


These dedicated runtime systems offer a very good platform to quickly develop scheduling policies that take into account
aspect such as memory restrictions and power budgeting.  In this work we seek to address issues that arise from shared resource contention, such as memory and power,  by developing
a methodology and scheduling policy, which can be incorporated into such a runtime system.  For memory, We develop a resource-aware
scheduler which identifies memory intensive tasks and avoids scheduling scenarios that would saturate memory bandwidth or increase
cache misses. For power, we develop a different scheduling policy which monitors the performance of a parallel application when running under power constrained environments. In this scenarion manufacturing issues cause same chip technology CPUs to run at different frequencies, under the same power constrained, creating a form of hetergeneity in the system.  Our scheduling-policy redistributes available power
and number of active cores among socket on the same node, in order to optimize performance by balancing resource sharing and mitigating the manufacturing variability observed.

