\begin{abstract}
    Parallelism sees an ever-growing adoption in an ever-expanding number of 
    fields. Modern HPC systems are designed to be massively parallel in mind 
    where an immense amount of computational units are available. Yet it proves 
    to be difficult to design parallel algorithms because not all the regions of 
    the program can be parallelized. Furthermore, many problems can only be 
    split into sub-problems with inter-dependencies. The negative impact of 
    communication is not negligible beyond merely a couple of processes.
    
    This thesis provides communication-reduction solutions on three problems in 
    the field of numerical methods and deep learning. We first set out to speed 
    up one of the iterative Krylov methods, the Conjugate Gradient Methods. This 
    work intends to fuse iterations together and thus defer the need for 
    synchronization at the end of the fuse phase. This approach also impedes the 
    application of some error correction routines. This thesis explores the 
    possibility to fuse iterations and its implication on the convergence of the 
    entire algorithm. Empirical evidences indicate that it achieves speedups 
    without hampering the convergence of the algorithm. 

    We then move on to DNN training with multiple GPUs in which the up-to-date 
    parameters stored and updated on the hosting CPU have to constantly 
    transferred to each GPU at the beginning of each batch. We propose to use a 
    dynamic scheme that compress the parameters to a lower precision on the CPU 
    side before the transfer takes place. The compress rate is guided by a 
    heuristics metric and the approach proceeds to increment the precision of 
    the parameters accordingly. It provides competitive accuracy while 
    outperforms our baseline in terms of training time.

    We eventually strive to improve the training of DNNs in a distributed-memory 
    system with model parallelism using message passing. By replicating the 
    neurons on each process once every two layers, we essentially cut the 
    communication in half during both the forward- and backward-propagation at 
    the cost of a 25\% increase in floating point computation. The trade-off 
    turns out to pay off according to our experiments and this approach 
    is able to offer significant speedups over the baseline approach where we 
    na\"{i}vely split the neurons at each layer.
\end{abstract}
